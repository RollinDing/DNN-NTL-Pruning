{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.0.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ruyi/anaconda3/envs/pytorch/lib/python3.9/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: filelock in /home/ruyi/anaconda3/envs/pytorch/lib/python3.9/site-packages (from gdown) (3.12.4)\n",
      "Requirement already satisfied: requests[socks] in /home/ruyi/anaconda3/envs/pytorch/lib/python3.9/site-packages (from gdown) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /home/ruyi/anaconda3/envs/pytorch/lib/python3.9/site-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ruyi/anaconda3/envs/pytorch/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ruyi/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests[socks]->gdown) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ruyi/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ruyi/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ruyi/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests[socks]->gdown) (2023.11.17)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/ruyi/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading gdown-5.0.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_id_by_model(folder_name):\n",
    "  file_id = {'resnet18_100-epochs_stl10': '14_nH2FkyKbt61cieQDiSbBVNP8-gtwgF',\n",
    "             'resnet18_100-epochs_cifar10': '1lc2aoVtrAetGn0PnTkOyFzPCIucOJq7C',\n",
    "             'resnet50_50-epochs_stl10': '1ByTKAUsdm_X7tLcii6oAEl5qFRqRMZSu'}\n",
    "  return file_id.get(folder_name, \"Model not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18_100-epochs_stl10 14_nH2FkyKbt61cieQDiSbBVNP8-gtwgF\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'resnet18_100-epochs_stl10'\n",
    "file_id = get_file_id_by_model(folder_name)\n",
    "print(folder_name, file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=14_nH2FkyKbt61cieQDiSbBVNP8-gtwgF\n",
      "From (redirected): https://drive.google.com/uc?id=14_nH2FkyKbt61cieQDiSbBVNP8-gtwgF&confirm=t&uuid=2ce8dccc-6dc2-402e-9428-06179075ef31\n",
      "To: /mnt/2tb/ruyi/DNN-NTL-Pruning/src/ssl/resnet18_100-epochs_stl10.zip\n",
      "100%|██████████| 116M/116M [00:01<00:00, 92.3MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  resnet18_100-epochs_stl10.zip\n",
      "  inflating: checkpoint_0100.pth.tar  \n",
      "  inflating: config.yml              \n",
      "  inflating: events.out.tfevents.1610901470.4cb2c837708d.2683858.0  \n",
      "  inflating: training.log            \n",
      "checkpoint_0100.pth.tar\n",
      "config.yml\n",
      "events.out.tfevents.1610901470.4cb2c837708d.2683858.0\n",
      "prune-ssl-model.py\n",
      "resnet18_100-epochs_stl10.zip\n",
      "simclr.ipynb\n",
      "training.log\n"
     ]
    }
   ],
   "source": [
    "# download and extract model files\n",
    "os.system('gdown https://drive.google.com/uc?id={}'.format(file_id))\n",
    "os.system('unzip {}'.format(folder_name))\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stl10_data_loaders(download, batch_size=256):\n",
    "  train_dataset = datasets.STL10('../../data', split='train', download=download,\n",
    "                                  transform=transforms.ToTensor())\n",
    "\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                            num_workers=10, drop_last=False, shuffle=True)\n",
    "  \n",
    "  test_dataset = datasets.STL10('../../data', split='test', download=download,\n",
    "                                  transform=transforms.ToTensor())\n",
    "\n",
    "  test_loader = DataLoader(test_dataset, batch_size=2*batch_size,\n",
    "                            num_workers=10, drop_last=False, shuffle=False)\n",
    "  return train_loader, test_loader\n",
    "\n",
    "def get_cifar10_data_loaders(download, shuffle=False, batch_size=256):\n",
    "  train_dataset = datasets.CIFAR10('../../data', train=True, download=download,\n",
    "                                  transform=transforms.ToTensor())\n",
    "\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                            num_workers=10, drop_last=False, shuffle=True)\n",
    "  \n",
    "  test_dataset = datasets.CIFAR10('../../data', train=False, download=download,\n",
    "                                  transform=transforms.ToTensor())\n",
    "\n",
    "  test_loader = DataLoader(test_dataset, batch_size=2*batch_size,\n",
    "                            num_workers=10, drop_last=False, shuffle=False)\n",
    "  return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar_dataloader(ratio=1.0):\n",
    "    \"\"\"\n",
    "    Get the CIFAR10 dataloader\n",
    "    \"\"\"\n",
    "    # Data loading code for cifar10 \n",
    "    train_transform = transforms.transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.transforms.RandomHorizontalFlip(),\n",
    "        transforms.transforms.ToTensor(),\n",
    "        transforms.transforms.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.2023, 0.1994, 0.2010],\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.transforms.ToTensor(),\n",
    "        transforms.transforms.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.2023, 0.1994, 0.2010],\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=\"../../data/\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    # Define the size of the subset\n",
    "    subset_size = int(len(train_dataset) * ratio)\n",
    "    print(f\"Using the sample size of {subset_size}.\")\n",
    "\n",
    "    # Create a random subset for training\n",
    "    indices = np.random.permutation(len(train_dataset))\n",
    "    train_indices = indices[:subset_size]\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "\n",
    "    val_dataset = datasets.CIFAR10(\n",
    "        root=\"../../data/\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        num_workers=10,\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=10,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruyi/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ruyi/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=False, num_classes=10).to(device)\n",
    "checkpoint = torch.load('../../base_models/resnet18-simclr-cifar10.tar', map_location=device)\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "for k in list(state_dict.keys()):\n",
    "\n",
    "  if k.startswith('backbone.'):\n",
    "    if k.startswith('backbone') and not k.startswith('backbone.fc'):\n",
    "      # remove prefix\n",
    "      state_dict[k[len(\"backbone.\"):]] = state_dict[k]\n",
    "  del state_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using the sample size of 50000.\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "log = model.load_state_dict(state_dict, strict=False)\n",
    "assert log.missing_keys == ['fc.weight', 'fc.bias']\n",
    "train_loader, test_loader = get_cifar_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=0.0008)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\tTop1 Train accuracy 59.30364990234375\tTop1 Test accuracy: 68.212890625\tTop5 Train accuracy: 97.109375\t\n",
      "Epoch 1\tTop1 Train accuracy 72.28834533691406\tTop1 Test accuracy: 71.9921875\tTop5 Train accuracy: 97.724609375\t\n",
      "Epoch 2\tTop1 Train accuracy 77.15800476074219\tTop1 Test accuracy: 74.3359375\tTop5 Train accuracy: 98.125\t\n",
      "Epoch 3\tTop1 Train accuracy 80.27742004394531\tTop1 Test accuracy: 74.39453125\tTop5 Train accuracy: 98.251953125\t\n",
      "Epoch 4\tTop1 Train accuracy 82.39835357666016\tTop1 Test accuracy: 75.322265625\tTop5 Train accuracy: 98.271484375\t\n",
      "Epoch 5\tTop1 Train accuracy 84.78356170654297\tTop1 Test accuracy: 76.26953125\tTop5 Train accuracy: 98.349609375\t\n",
      "Epoch 6\tTop1 Train accuracy 86.57565307617188\tTop1 Test accuracy: 76.044921875\tTop5 Train accuracy: 98.30078125\t\n",
      "Epoch 7\tTop1 Train accuracy 87.97512817382812\tTop1 Test accuracy: 75.99609375\tTop5 Train accuracy: 98.3203125\t\n",
      "Epoch 8\tTop1 Train accuracy 89.07206726074219\tTop1 Test accuracy: 77.255859375\tTop5 Train accuracy: 98.427734375\t\n",
      "Epoch 9\tTop1 Train accuracy 90.67083740234375\tTop1 Test accuracy: 75.95703125\tTop5 Train accuracy: 98.310546875\t\n",
      "Epoch 10\tTop1 Train accuracy 91.42418670654297\tTop1 Test accuracy: 76.337890625\tTop5 Train accuracy: 98.22265625\t\n",
      "Epoch 11\tTop1 Train accuracy 92.3816146850586\tTop1 Test accuracy: 76.23046875\tTop5 Train accuracy: 98.4375\t\n",
      "Epoch 12\tTop1 Train accuracy 92.95639038085938\tTop1 Test accuracy: 76.103515625\tTop5 Train accuracy: 98.251953125\t\n",
      "Epoch 13\tTop1 Train accuracy 93.72767639160156\tTop1 Test accuracy: 76.89453125\tTop5 Train accuracy: 98.359375\t\n",
      "Epoch 14\tTop1 Train accuracy 94.1589584350586\tTop1 Test accuracy: 76.611328125\tTop5 Train accuracy: 98.115234375\t\n",
      "Epoch 15\tTop1 Train accuracy 94.47066497802734\tTop1 Test accuracy: 76.40625\tTop5 Train accuracy: 98.193359375\t\n",
      "Epoch 16\tTop1 Train accuracy 95.14947509765625\tTop1 Test accuracy: 76.62109375\tTop5 Train accuracy: 98.349609375\t\n",
      "Epoch 17\tTop1 Train accuracy 95.29734802246094\tTop1 Test accuracy: 76.69921875\tTop5 Train accuracy: 98.28125\t\n",
      "Epoch 18\tTop1 Train accuracy 95.56282043457031\tTop1 Test accuracy: 76.630859375\tTop5 Train accuracy: 98.193359375\t\n",
      "Epoch 19\tTop1 Train accuracy 95.86176300048828\tTop1 Test accuracy: 76.640625\tTop5 Train accuracy: 98.388671875\t\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "  top1_train_accuracy = 0\n",
    "  for counter, (x_batch, y_batch) in enumerate(train_loader):\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "\n",
    "    logits = model(x_batch)\n",
    "    loss = criterion(logits, y_batch)\n",
    "    \n",
    "    top1 = accuracy(logits, y_batch, topk=(1,))\n",
    "    top1_train_accuracy += top1[0]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  top1_train_accuracy /= (counter + 1)\n",
    "  top1_accuracy = 0\n",
    "  top5_accuracy = 0\n",
    "  for counter, (x_batch, y_batch) in enumerate(test_loader):\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "\n",
    "    logits = model(x_batch)\n",
    "  \n",
    "    top1, top5 = accuracy(logits, y_batch, topk=(1,5))\n",
    "    top1_accuracy += top1[0]\n",
    "    top5_accuracy += top5[0]\n",
    "  \n",
    "  top1_accuracy /= (counter + 1)\n",
    "  top5_accuracy /= (counter + 1)\n",
    "  print(f\"Epoch {epoch}\\tTop1 Train accuracy {top1_train_accuracy.item()}\\tTop1 Test accuracy: {top1_accuracy.item()}\\tTop5 Train accuracy: {top5_accuracy.item()}\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after fine-tuning\n",
    "model_path = \"../../base_models/resnet18-finetune-cifar10.tar\" \n",
    "# save the model static dict \n",
    "torch.save({\n",
    "    'state_dict': model.state_dict(),\n",
    "}, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
